{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ijT1UJe0msP",
        "outputId": "c797d012-d30f-4161-e2d7-7f98b9dc592a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Poetry\n",
            "0  aañkh se duur na ho dil se utar jā.egā \\nvaqt ...\n",
            "1  āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo \\n...\n",
            "2  ab aur kyā kisī se marāsim baḌhā.eñ ham \\nye b...\n",
            "3  ab ke ham bichhḌe to shāyad kabhī ḳhvāboñ meñ ...\n",
            "4  ab ke tajdīd-e-vafā kā nahīñ imkāñ jānāñ \\nyaa...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read only the \"Poetry\" column from the CSV file\n",
        "df = pd.read_csv(\"Roman-Urdu-Poetry.csv\", usecols=[\"Poetry\"])\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to remove Urdu punctuation and diacritics\n",
        "def remove_urdu_punctuation_and_diacritics(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove Urdu punctuation and diacritics\n",
        "        text = re.sub(r'[،۔؟!\"“”‘’؛\\.-]', '', text)  # Removes punctuation\n",
        "        return text\n",
        "    return text\n",
        "\n",
        "# Clean specific columns\n",
        "columns_to_clean = ['Poetry']\n",
        "\n",
        "for column in columns_to_clean:\n",
        "    df[column] = df[column].apply(remove_urdu_punctuation_and_diacritics)\n",
        "\n",
        "df.to_csv(\"Cleaned_Roman-Urdu-Poetry.csv\", index=False)\n",
        "\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKGCsVnp05WX",
        "outputId": "3964a8c9-08f7-435c-c4c6-2bc233a8f4ad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Poetry\n",
            "0  aañkh se duur na ho dil se utar jāegā \\nvaqt k...\n",
            "1  āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo \\n...\n",
            "2  ab aur kyā kisī se marāsim baḌhāeñ ham \\nye bh...\n",
            "3  ab ke ham bichhḌe to shāyad kabhī ḳhvāboñ meñ ...\n",
            "4  ab ke tajdīdevafā kā nahīñ imkāñ jānāñ \\nyaad ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Convert poetry to a list\n",
        "poetry_lines = df[\"Poetry\"].tolist()\n",
        "\n",
        "# Tokenizer to convert words into numerical form\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(poetry_lines)\n",
        "\n",
        "# Convert text into sequences\n",
        "sequences = tokenizer.texts_to_sequences(poetry_lines)\n",
        "\n",
        "# Define input (X) and output (Y) sequences\n",
        "input_sequences = []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        input_sequences.append(seq[:i+1])\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\")\n",
        "\n",
        "# Split into X (input) and Y (output)\n",
        "X, Y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Convert Y into one-hot encoding\n",
        "Y = np.array(Y)  # Keep Y as integer labels\n",
        "\n",
        "print(\"Data prepared for LSTM training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCTRpgGv071E",
        "outputId": "d4db7fa4-2d20-4cc6-ea1b-e2359fd7764f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data prepared for LSTM training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split data into Training (80%) and Temporary Set (20%)\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the Temporary Set into Validation (10%) and Testing (10%)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Data successfully split!\")\n",
        "print(f\"Training Data: X_train = {X_train.shape}, Y_train = {Y_train.shape}\")\n",
        "print(f\"Validation Data: X_val = {X_val.shape}, Y_val = {Y_val.shape}\")\n",
        "print(f\"Test Data: X_test = {X_test.shape}, Y_test = {Y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5OFQj4o0-Ve",
        "outputId": "f965cb84-752f-4701-f260-c62a4520cba9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully split!\n",
            "Training Data: X_train = (124496, 433), Y_train = (124496,)\n",
            "Validation Data: X_val = (15562, 433), Y_val = (15562,)\n",
            "Test Data: X_test = (15563, 433), Y_test = (15563,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 1: Get Vocabulary Size\n",
        "vocab_size = np.max(X_train) + 1  # Largest token ID + 1\n",
        "\n",
        "# Step 2: Define the LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=X_train.shape[1]),  # Embedding layer\n",
        "    LSTM(150, return_sequences=True),  # First LSTM layer\n",
        "    LSTM(150),  # Second LSTM layer\n",
        "    Dense(150, activation=\"relu\"),  # Dense layer\n",
        "    Dense(vocab_size, activation=\"softmax\")  # Output layer\n",
        "])\n",
        "\n",
        "# Step 3: Compile the Model\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Step 4: Train the Model\n",
        "epochs = 55  # Adjust based on performance\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 5: Save the Model\n",
        "model.save(\"lstm_poetry_model.h5\")\n",
        "print(\"Model trained and saved as 'lstm_poetry_model.h5'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyv5cvTE1A0V",
        "outputId": "c5024617-0762-4ab6-d3b1-33bd8a21efd2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 86ms/step - accuracy: 0.0415 - loss: 7.3163 - val_accuracy: 0.0419 - val_loss: 6.8869\n",
            "Epoch 2/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 85ms/step - accuracy: 0.0445 - loss: 6.6624 - val_accuracy: 0.0459 - val_loss: 6.9709\n",
            "Epoch 3/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 87ms/step - accuracy: 0.0518 - loss: 6.5301 - val_accuracy: 0.0547 - val_loss: 6.9744\n",
            "Epoch 4/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 88ms/step - accuracy: 0.0647 - loss: 6.3574 - val_accuracy: 0.0645 - val_loss: 7.0290\n",
            "Epoch 5/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 85ms/step - accuracy: 0.0753 - loss: 6.1868 - val_accuracy: 0.0659 - val_loss: 7.0826\n",
            "Epoch 6/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.0859 - loss: 6.0295 - val_accuracy: 0.0700 - val_loss: 7.1175\n",
            "Epoch 7/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.0954 - loss: 5.8814 - val_accuracy: 0.0734 - val_loss: 7.2046\n",
            "Epoch 8/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 86ms/step - accuracy: 0.1022 - loss: 5.7634 - val_accuracy: 0.0752 - val_loss: 7.3953\n",
            "Epoch 9/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.1107 - loss: 5.6352 - val_accuracy: 0.0749 - val_loss: 7.5136\n",
            "Epoch 10/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.1186 - loss: 5.4811 - val_accuracy: 0.0771 - val_loss: 7.6141\n",
            "Epoch 11/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 86ms/step - accuracy: 0.1261 - loss: 5.3565 - val_accuracy: 0.0779 - val_loss: 7.8481\n",
            "Epoch 12/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.1347 - loss: 5.2153 - val_accuracy: 0.0778 - val_loss: 8.1449\n",
            "Epoch 13/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.1430 - loss: 5.0860 - val_accuracy: 0.0787 - val_loss: 8.3838\n",
            "Epoch 14/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.1488 - loss: 4.9587 - val_accuracy: 0.0766 - val_loss: 8.7486\n",
            "Epoch 15/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.1554 - loss: 4.8270 - val_accuracy: 0.0767 - val_loss: 9.1629\n",
            "Epoch 16/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.1671 - loss: 4.6763 - val_accuracy: 0.0795 - val_loss: 9.6117\n",
            "Epoch 17/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.1728 - loss: 4.5648 - val_accuracy: 0.0750 - val_loss: 10.0630\n",
            "Epoch 18/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 86ms/step - accuracy: 0.1849 - loss: 4.4294 - val_accuracy: 0.0751 - val_loss: 10.5085\n",
            "Epoch 19/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 87ms/step - accuracy: 0.1987 - loss: 4.3230 - val_accuracy: 0.0731 - val_loss: 11.0286\n",
            "Epoch 20/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.2112 - loss: 4.2316 - val_accuracy: 0.0736 - val_loss: 11.4299\n",
            "Epoch 21/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.2248 - loss: 4.1221 - val_accuracy: 0.0724 - val_loss: 11.8920\n",
            "Epoch 22/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 86ms/step - accuracy: 0.2365 - loss: 4.0382 - val_accuracy: 0.0705 - val_loss: 12.2463\n",
            "Epoch 23/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.2491 - loss: 3.9554 - val_accuracy: 0.0707 - val_loss: 12.6161\n",
            "Epoch 24/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.2594 - loss: 3.8739 - val_accuracy: 0.0695 - val_loss: 13.0948\n",
            "Epoch 25/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.2674 - loss: 3.8097 - val_accuracy: 0.0677 - val_loss: 13.2710\n",
            "Epoch 26/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.2780 - loss: 3.7341 - val_accuracy: 0.0696 - val_loss: 13.7946\n",
            "Epoch 27/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 87ms/step - accuracy: 0.2899 - loss: 3.6542 - val_accuracy: 0.0679 - val_loss: 14.1161\n",
            "Epoch 28/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 85ms/step - accuracy: 0.3011 - loss: 3.6017 - val_accuracy: 0.0670 - val_loss: 14.4706\n",
            "Epoch 29/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 87ms/step - accuracy: 0.3100 - loss: 3.5462 - val_accuracy: 0.0678 - val_loss: 14.8024\n",
            "Epoch 30/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.3170 - loss: 3.4962 - val_accuracy: 0.0663 - val_loss: 15.0698\n",
            "Epoch 31/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 87ms/step - accuracy: 0.3280 - loss: 3.4290 - val_accuracy: 0.0643 - val_loss: 15.1943\n",
            "Epoch 32/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 85ms/step - accuracy: 0.3348 - loss: 3.3694 - val_accuracy: 0.0632 - val_loss: 15.7889\n",
            "Epoch 33/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 86ms/step - accuracy: 0.3436 - loss: 3.3299 - val_accuracy: 0.0638 - val_loss: 16.0927\n",
            "Epoch 34/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 86ms/step - accuracy: 0.3492 - loss: 3.2877 - val_accuracy: 0.0625 - val_loss: 16.3287\n",
            "Epoch 35/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 85ms/step - accuracy: 0.3560 - loss: 3.2354 - val_accuracy: 0.0638 - val_loss: 16.5310\n",
            "Epoch 36/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 86ms/step - accuracy: 0.3676 - loss: 3.1735 - val_accuracy: 0.0627 - val_loss: 16.8574\n",
            "Epoch 37/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.3726 - loss: 3.1330 - val_accuracy: 0.0609 - val_loss: 16.9845\n",
            "Epoch 38/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 85ms/step - accuracy: 0.3808 - loss: 3.0855 - val_accuracy: 0.0616 - val_loss: 17.4145\n",
            "Epoch 39/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.3887 - loss: 3.0488 - val_accuracy: 0.0605 - val_loss: 17.6279\n",
            "Epoch 40/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 86ms/step - accuracy: 0.3949 - loss: 2.9996 - val_accuracy: 0.0634 - val_loss: 17.9477\n",
            "Epoch 41/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.3999 - loss: 2.9608 - val_accuracy: 0.0610 - val_loss: 17.9941\n",
            "Epoch 42/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 86ms/step - accuracy: 0.4100 - loss: 2.9198 - val_accuracy: 0.0601 - val_loss: 18.5916\n",
            "Epoch 43/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 86ms/step - accuracy: 0.4135 - loss: 2.8864 - val_accuracy: 0.0603 - val_loss: 18.7660\n",
            "Epoch 44/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 85ms/step - accuracy: 0.4225 - loss: 2.8374 - val_accuracy: 0.0616 - val_loss: 19.1917\n",
            "Epoch 45/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4280 - loss: 2.8085 - val_accuracy: 0.0600 - val_loss: 19.1805\n",
            "Epoch 46/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4345 - loss: 2.7573 - val_accuracy: 0.0597 - val_loss: 19.4474\n",
            "Epoch 47/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4412 - loss: 2.7246 - val_accuracy: 0.0571 - val_loss: 19.7172\n",
            "Epoch 48/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 85ms/step - accuracy: 0.4512 - loss: 2.6748 - val_accuracy: 0.0610 - val_loss: 19.9618\n",
            "Epoch 49/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4559 - loss: 2.6455 - val_accuracy: 0.0589 - val_loss: 20.2474\n",
            "Epoch 50/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4630 - loss: 2.6150 - val_accuracy: 0.0569 - val_loss: 20.3708\n",
            "Epoch 51/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4685 - loss: 2.5757 - val_accuracy: 0.0562 - val_loss: 20.6509\n",
            "Epoch 52/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4714 - loss: 2.5517 - val_accuracy: 0.0571 - val_loss: 20.8723\n",
            "Epoch 53/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4774 - loss: 2.5175 - val_accuracy: 0.0585 - val_loss: 21.2016\n",
            "Epoch 54/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 85ms/step - accuracy: 0.4858 - loss: 2.4776 - val_accuracy: 0.0574 - val_loss: 21.2510\n",
            "Epoch 55/55\n",
            "\u001b[1m973/973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 86ms/step - accuracy: 0.4883 - loss: 2.4580 - val_accuracy: 0.0564 - val_loss: 21.5379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved as 'lstm_poetry_model.h5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 1: Load the Trained Model\n",
        "model = tf.keras.models.load_model(\"lstm_poetry_model.h5\")\n",
        "\n",
        "# Fix the Warning: Recompile the Model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
        "\n",
        "# Step 2: Load the Tokenizer\n",
        "data_path = \"Roman-Urdu-Poetry.csv\"  # Ensure this is the same dataset used before\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Combine all poetry to recreate the tokenizer\n",
        "all_poetry = \" \".join(df[\"Poetry\"].astype(str).tolist()).lower()\n",
        "\n",
        "# Tokenize again (ensure consistency with training)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([all_poetry])\n",
        "\n",
        "# Get sequence length from training\n",
        "sequence_length = 10  # Ensure this matches the training sequence length\n",
        "\n",
        "# ✅ Step 3: Function to Generate Poetry with Diversity\n",
        "def generate_poetry(seed_text, next_words, model, tokenizer, sequence_length, temperature=1.0):\n",
        "    generated_words = set()  # Store generated words to avoid excessive repetition\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Convert seed text to numerical tokens\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')\n",
        "\n",
        "        # Predict next word probabilities\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        predicted_probs = np.log(predicted_probs + 1e-8) / temperature  # Avoid log(0)\n",
        "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))  # Softmax\n",
        "\n",
        "        # Sample the next word instead of always choosing the highest probability\n",
        "        predicted_word_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "\n",
        "        # Convert token ID back to word\n",
        "        next_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                next_word = word\n",
        "                break\n",
        "\n",
        "        # If no valid word is found, stop generation\n",
        "        if not next_word or next_word in generated_words:\n",
        "            continue  # Skip repeated words\n",
        "\n",
        "        generated_words.add(next_word)\n",
        "        seed_text += \" \" + next_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# ✅ Step 4: Generate Poetry\n",
        "seed_text = \"muj se pehli se mohabbat\"  # Provide some starting words\n",
        "generated_poetry = generate_poetry(seed_text, 50, model, tokenizer, sequence_length, temperature=0.8)\n",
        "\n",
        "print(\"📝 Generated Poetry:\\n\", generated_poetry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W07Q5KKT9HwT",
        "outputId": "07dfc13e-ad43-4563-a2a7-6b27d8bfff51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Generated Poetry:\n",
            " muj se pehli se mohabbat sunā hai hī paanī haiñ kaam husn jis ye se ko ki nahīñ gayā maiñ āzād thī kī nigāh bhī supurd aur 'farāz' firāq e vo jāntā uchatte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHy9iX127eXv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5hqLLUM7iRb"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}